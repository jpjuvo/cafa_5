{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a32ba1b4",
   "metadata": {
    "papermill": {
     "duration": 0.009356,
     "end_time": "2023-08-11T07:26:39.891137",
     "exception": false,
     "start_time": "2023-08-11T07:26:39.881781",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# CAFA5 DeepGoZero\n",
    "\n",
    "Based on Geraseva's original notebook.\n",
    "Adapted for inference on CAFA5 dataset \n",
    "\n",
    "### Instructions\n",
    "For each Ontology class, a submission must be generated. \n",
    "Go to Hyperparamter Definition, and run once for each ONT class. You will then need to do some manual postprocessing to combine all 3 submissions.\n",
    "Note, the test dataset is currently running for a 100-protein sample for memory purposes. You will find the commented out line relevant.\n",
    "It may be, depending on your specs, that you need a test dataloader. An example of that is provided in the inference cells.\n",
    "\n",
    "Note that there are three levels of GO-fidelity provided in Geraseva's dataset (NB THIS IS NOT CAFA5 BUT ORIGINAL REPO). These are also discussed under Hyperparameter Definition.\n",
    "TODO: Diamond + Blast predictions are provided in Gerasevas data. If you have your own diamond predictions, you can combine that as well.\n",
    "A link to a example in DeepGoPlus do this is provided in the final cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82fe492a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-11T07:26:39.907587Z",
     "iopub.status.busy": "2023-08-11T07:26:39.907146Z",
     "iopub.status.idle": "2023-08-11T07:26:40.020843Z",
     "shell.execute_reply": "2023-08-11T07:26:40.019869Z"
    },
    "papermill": {
     "duration": 0.12606,
     "end_time": "2023-08-11T07:26:40.024697",
     "exception": false,
     "start_time": "2023-08-11T07:26:39.898637",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93ed4b46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-11T07:26:40.042569Z",
     "iopub.status.busy": "2023-08-11T07:26:40.041182Z",
     "iopub.status.idle": "2023-08-11T07:26:40.046137Z",
     "shell.execute_reply": "2023-08-11T07:26:40.045267Z"
    },
    "papermill": {
     "duration": 0.015403,
     "end_time": "2023-08-11T07:26:40.048040",
     "exception": false,
     "start_time": "2023-08-11T07:26:40.032637",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0dd62e95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-11T07:26:53.386824Z",
     "iopub.status.busy": "2023-08-11T07:26:53.386486Z",
     "iopub.status.idle": "2023-08-11T07:26:58.755148Z",
     "shell.execute_reply": "2023-08-11T07:26:58.754148Z"
    },
    "papermill": {
     "duration": 5.38091,
     "end_time": "2023-08-11T07:26:58.757586",
     "exception": false,
     "start_time": "2023-08-11T07:26:53.376676",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch as th\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from sklearn.metrics import roc_curve, auc, matthews_corrcoef\n",
    "import copy\n",
    "from torch.utils.data import DataLoader, IterableDataset, TensorDataset\n",
    "from itertools import cycle\n",
    "import math\n",
    "from dgl.nn import GraphConv, GATConv\n",
    "import dgl\n",
    "from collections import deque, Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d377d7",
   "metadata": {
    "papermill": {
     "duration": 0.009939,
     "end_time": "2023-08-11T07:26:58.776677",
     "exception": false,
     "start_time": "2023-08-11T07:26:58.766738",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Hyperparameter Definition\n",
    "\n",
    "* Note for retraining, GO Norm would have to be replaced with our own to match size. But irrelevant for inference.\n",
    "* With default terms and model file (deepgozero_zero_10.th & terms_zero_10.pkl), we observe 10490 annotations in output. Smaller than our 40K.\n",
    "* Note the extended comment on 3 models/terms files per GO class, allowing for max (3x3) inference run configurations. See comment for details\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9df9dbdb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-11T07:26:58.795826Z",
     "iopub.status.busy": "2023-08-11T07:26:58.795169Z",
     "iopub.status.idle": "2023-08-11T07:26:58.815001Z",
     "shell.execute_reply": "2023-08-11T07:26:58.814112Z"
    },
    "papermill": {
     "duration": 0.032178,
     "end_time": "2023-08-11T07:26:58.817272",
     "exception": false,
     "start_time": "2023-08-11T07:26:58.785094",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_root='../input/deepgozero-data/data'\n",
    "ont='bp'  # Ontology class definition, because DGZ has models for each class. mf, bp, or cc\n",
    "device='cpu' #'cuda:0'\n",
    "batch_size=37 # batch size currently not used for inference, training only\n",
    "epochs=256 # epochs currently not used for inference, training only\n",
    "load=True # If False, retrain. Note not relevant for CAFA5, keep True for inference\n",
    "go_file = f'{data_root}/go.norm' # GO DGZ file. contains 10490 GO annotations\n",
    "\"\"\"\n",
    "Note, for each GO ontology class, we we have 3 possible model and terms files\n",
    "- deepgozero.th\n",
    "- deepgozero_zero.th\n",
    "- deepgozero_zero_10.th\n",
    "- terms.pkl\n",
    "- terms_zero.pkl\n",
    "- terms_zero_10.pkl\n",
    "\n",
    "These correspond to different sets of GO terms to train with. For example for GO=MF\n",
    "- len(terms) = 6868\n",
    "- len(terms_zero) =6863\n",
    "- len(terms_zero_10)=2041\n",
    "\n",
    "The notebook default are the packages ending with zero_10, also consumes the least memory.\n",
    "However it may be good to test out the other ones, if time allows\n",
    "\"\"\"\n",
    "terms_mode = 0 # 0,1 or 2\n",
    "\n",
    "if terms_mode == 0:\n",
    "    model_file = f'{data_root}/{ont}/deepgozero_zero_10.th' # Model file\n",
    "    terms_file = f'{data_root}/{ont}/terms_zero_10.pkl'\n",
    "elif terms_mode == 1:\n",
    "    model_file = f'{data_root}/{ont}/deepgozero_zero.th' # Model file\n",
    "    terms_file = f'{data_root}/{ont}/terms_zero.pkl'\n",
    "elif terms_mode == 2:\n",
    "    model_file = f'{data_root}/{ont}/deepgozero.th' # Model file\n",
    "    terms_file = f'{data_root}/{ont}/terms.pkl'\n",
    "\n",
    "out_file = \"../output/DGZ/predictions_deepgozero_zero_10.pkl\"\n",
    "test_df = pd.read_csv('../input/cafa-fasta-4/test_df.csv') # instead of taking the whole dataset, we will take a 100 row sample\n",
    "#test_df= pd.read_csv('../input/cafa-fasta-4/test_sample.csv') # CAFA5 data test dataframe,  generated using https://github.com/bio-ontology-research-group/deepgozero/blob/main/interpro_data.py\n",
    "threshold=0.1 # Probability threshold to accept prediction\n",
    "\n",
    "sub_file = f'../output/DGZ/{ont}_th-{threshold}_{os.path.basename(model_file)[:-3]}_submission.tsv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4c3ae0",
   "metadata": {
    "papermill": {
     "duration": 0.008252,
     "end_time": "2023-08-11T07:26:58.834398",
     "exception": false,
     "start_time": "2023-08-11T07:26:58.826146",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## DeepGoZero repository code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3c9056",
   "metadata": {
    "papermill": {
     "duration": 0.00813,
     "end_time": "2023-08-11T07:26:58.851354",
     "exception": false,
     "start_time": "2023-08-11T07:26:58.843224",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We use code from https://github.com/bio-ontology-research-group/deepgozero. \n",
    "Instead of cloning the repo we copy all necessary code into the notebook and run it.\n",
    "\n",
    "Note, insufficient for CAFA5 retraining, only inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9ea6caf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-11T07:26:58.870253Z",
     "iopub.status.busy": "2023-08-11T07:26:58.869308Z",
     "iopub.status.idle": "2023-08-11T07:26:58.891201Z",
     "shell.execute_reply": "2023-08-11T07:26:58.890326Z"
    },
    "papermill": {
     "duration": 0.033719,
     "end_time": "2023-08-11T07:26:58.893363",
     "exception": false,
     "start_time": "2023-08-11T07:26:58.859644",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from https://github.com/bio-ontology-research-group/deepgozero/blob/main/utils.py\n",
    "AALETTER = [\n",
    "    'A', 'R', 'N', 'D', 'C', 'Q', 'E', 'G', 'H', 'I',\n",
    "    'L', 'K', 'M', 'F', 'P', 'S', 'T', 'W', 'Y', 'V']\n",
    "AANUM = len(AALETTER)\n",
    "AAINDEX = dict()\n",
    "for i in range(len(AALETTER)):\n",
    "    AAINDEX[AALETTER[i]] = i + 1\n",
    "INVALID_ACIDS = set(['U', 'O', 'B', 'Z', 'J', 'X', '*'])\n",
    "MAXLEN = 2000\n",
    "NGRAMS = {}\n",
    "for i in range(20):\n",
    "    for j in range(20):\n",
    "        for k in range(20):\n",
    "            ngram = AALETTER[i] + AALETTER[j] + AALETTER[k]\n",
    "            index = 400 * i + 20 * j + k + 1\n",
    "            NGRAMS[ngram] = index\n",
    "\n",
    "def is_ok(seq):\n",
    "    for c in seq:\n",
    "        if c in INVALID_ACIDS:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def to_ngrams(seq):\n",
    "    l = min(MAXLEN, len(seq) - 3)\n",
    "    ngrams = np.zeros((l,), dtype=np.int32)\n",
    "    for i in range(l):\n",
    "        ngrams[i] = NGRAMS.get(seq[i: i + 3], 0)\n",
    "    return ngrams\n",
    "\n",
    "def to_tokens(seq):\n",
    "    tokens = np.zeros((MAXLEN, ), dtype=np.float32)\n",
    "    l = min(MAXLEN, len(seq))\n",
    "    for i in range(l):\n",
    "        tokens[i] = AAINDEX.get(seq[i], 0)\n",
    "    return tokens\n",
    "\n",
    "def to_onehot(seq, start=0):\n",
    "    onehot = np.zeros((21, MAXLEN), dtype=np.float32)\n",
    "    l = min(MAXLEN, len(seq))\n",
    "    for i in range(start, start + l):\n",
    "        onehot[AAINDEX.get(seq[i - start], 0), i] = 1\n",
    "    onehot[0, 0:start] = 1\n",
    "    onehot[0, start + l:] = 1\n",
    "    return onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d839ed9",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-08-11T07:26:58.912058Z",
     "iopub.status.busy": "2023-08-11T07:26:58.911720Z",
     "iopub.status.idle": "2023-08-11T07:26:58.940381Z",
     "shell.execute_reply": "2023-08-11T07:26:58.939463Z"
    },
    "papermill": {
     "duration": 0.040796,
     "end_time": "2023-08-11T07:26:58.942621",
     "exception": false,
     "start_time": "2023-08-11T07:26:58.901825",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from https://github.com/bio-ontology-research-group/deepgozero/blob/main/utils.py\n",
    "# Ontology hierarchy handling methods\n",
    "class Ontology(object):\n",
    "\n",
    "    def __init__(self, filename='data/go.obo', with_rels=False):\n",
    "        self.ont = self.load(filename, with_rels)\n",
    "        self.ic = None\n",
    "        self.ic_norm = 0.0\n",
    "\n",
    "    def has_term(self, term_id):\n",
    "        return term_id in self.ont\n",
    "\n",
    "    def get_term(self, term_id):\n",
    "        if self.has_term(term_id):\n",
    "            return self.ont[term_id]\n",
    "        return None\n",
    "\n",
    "    def calculate_ic(self, annots):\n",
    "        cnt = Counter()\n",
    "        for x in annots:\n",
    "            cnt.update(x)\n",
    "        self.ic = {}\n",
    "        for go_id, n in cnt.items():\n",
    "            parents = self.get_parents(go_id)\n",
    "            if len(parents) == 0:\n",
    "                min_n = n\n",
    "            else:\n",
    "                min_n = min([cnt[x] for x in parents])\n",
    "\n",
    "            self.ic[go_id] = math.log(min_n / n, 2)\n",
    "            self.ic_norm = max(self.ic_norm, self.ic[go_id])\n",
    "    \n",
    "    def get_ic(self, go_id):\n",
    "        if self.ic is None:\n",
    "            raise Exception('Not yet calculated')\n",
    "        if go_id not in self.ic:\n",
    "            return 0.0\n",
    "        return self.ic[go_id]\n",
    "\n",
    "    def get_norm_ic(self, go_id):\n",
    "        return self.get_ic(go_id) / self.ic_norm\n",
    "\n",
    "    def load(self, filename, with_rels):\n",
    "        ont = dict()\n",
    "        obj = None\n",
    "        with open(filename, 'r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                if line == '[Term]':\n",
    "                    if obj is not None:\n",
    "                        ont[obj['id']] = obj\n",
    "                    obj = dict()\n",
    "                    obj['is_a'] = list()\n",
    "                    obj['part_of'] = list()\n",
    "                    obj['regulates'] = list()\n",
    "                    obj['alt_ids'] = list()\n",
    "                    obj['is_obsolete'] = False\n",
    "                    continue\n",
    "                elif line == '[Typedef]':\n",
    "                    if obj is not None:\n",
    "                        ont[obj['id']] = obj\n",
    "                    obj = None\n",
    "                else:\n",
    "                    if obj is None:\n",
    "                        continue\n",
    "                    l = line.split(\": \")\n",
    "                    if l[0] == 'id':\n",
    "                        obj['id'] = l[1]\n",
    "                    elif l[0] == 'alt_id':\n",
    "                        obj['alt_ids'].append(l[1])\n",
    "                    elif l[0] == 'namespace':\n",
    "                        obj['namespace'] = l[1]\n",
    "                    elif l[0] == 'is_a':\n",
    "                        obj['is_a'].append(l[1].split(' ! ')[0])\n",
    "                    elif with_rels and l[0] == 'relationship':\n",
    "                        it = l[1].split()\n",
    "                        # add all types of relationships\n",
    "                        obj['is_a'].append(it[1])\n",
    "                    elif l[0] == 'name':\n",
    "                        obj['name'] = l[1]\n",
    "                    elif l[0] == 'is_obsolete' and l[1] == 'true':\n",
    "                        obj['is_obsolete'] = True\n",
    "            if obj is not None:\n",
    "                ont[obj['id']] = obj\n",
    "        for term_id in list(ont.keys()):\n",
    "            for t_id in ont[term_id]['alt_ids']:\n",
    "                ont[t_id] = ont[term_id]\n",
    "            if ont[term_id]['is_obsolete']:\n",
    "                del ont[term_id]\n",
    "        for term_id, val in ont.items():\n",
    "            if 'children' not in val:\n",
    "                val['children'] = set()\n",
    "            for p_id in val['is_a']:\n",
    "                if p_id in ont:\n",
    "                    if 'children' not in ont[p_id]:\n",
    "                        ont[p_id]['children'] = set()\n",
    "                    ont[p_id]['children'].add(term_id)\n",
    "     \n",
    "        return ont\n",
    "\n",
    "    def get_anchestors(self, term_id):\n",
    "        if term_id not in self.ont:\n",
    "            return set()\n",
    "        term_set = set()\n",
    "        q = deque()\n",
    "        q.append(term_id)\n",
    "        while(len(q) > 0):\n",
    "            t_id = q.popleft()\n",
    "            if t_id not in term_set:\n",
    "                term_set.add(t_id)\n",
    "                for parent_id in self.ont[t_id]['is_a']:\n",
    "                    if parent_id in self.ont:\n",
    "                        q.append(parent_id)\n",
    "        return term_set\n",
    "\n",
    "    def get_prop_terms(self, terms):\n",
    "        prop_terms = set()\n",
    "\n",
    "        for term_id in terms:\n",
    "            prop_terms |= self.get_anchestors(term_id)\n",
    "        return prop_terms\n",
    "\n",
    "\n",
    "    def get_parents(self, term_id):\n",
    "        if term_id not in self.ont:\n",
    "            return set()\n",
    "        term_set = set()\n",
    "        for parent_id in self.ont[term_id]['is_a']:\n",
    "            if parent_id in self.ont:\n",
    "                term_set.add(parent_id)\n",
    "        return term_set\n",
    "\n",
    "\n",
    "    def get_namespace_terms(self, namespace):\n",
    "        terms = set()\n",
    "        for go_id, obj in self.ont.items():\n",
    "            if obj['namespace'] == namespace:\n",
    "                terms.add(go_id)\n",
    "        return terms\n",
    "\n",
    "    def get_namespace(self, term_id):\n",
    "        return self.ont[term_id]['namespace']\n",
    "    \n",
    "    def get_term_set(self, term_id):\n",
    "        if term_id not in self.ont:\n",
    "            return set()\n",
    "        term_set = set()\n",
    "        q = deque()\n",
    "        q.append(term_id)\n",
    "        while len(q) > 0:\n",
    "            t_id = q.popleft()\n",
    "            if t_id not in term_set:\n",
    "                term_set.add(t_id)\n",
    "                for ch_id in self.ont[t_id]['children']:\n",
    "                    q.append(ch_id)\n",
    "        return term_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4c32664",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-08-11T07:26:58.962047Z",
     "iopub.status.busy": "2023-08-11T07:26:58.961715Z",
     "iopub.status.idle": "2023-08-11T07:26:58.979858Z",
     "shell.execute_reply": "2023-08-11T07:26:58.978892Z"
    },
    "papermill": {
     "duration": 0.030606,
     "end_time": "2023-08-11T07:26:58.981981",
     "exception": false,
     "start_time": "2023-08-11T07:26:58.951375",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from https://github.com/bio-ontology-research-group/deepgozero/blob/main/deepgozero.py\n",
    "# Primary load data methods.  \n",
    "def compute_roc(labels, preds):\n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr, tpr, _ = roc_curve(labels.flatten(), preds.flatten())\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    return roc_auc\n",
    "\n",
    "def load_normal_forms(go_file, terms_dict):\n",
    "    nf1 = []\n",
    "    nf2 = []\n",
    "    nf3 = []\n",
    "    nf4 = []\n",
    "    relations = {}\n",
    "    zclasses = {}\n",
    "    \n",
    "    def get_index(go_id):\n",
    "        if go_id in terms_dict:\n",
    "            index = terms_dict[go_id]\n",
    "        elif go_id in zclasses:\n",
    "            index = zclasses[go_id]\n",
    "        else:\n",
    "            zclasses[go_id] = len(terms_dict) + len(zclasses)\n",
    "            index = zclasses[go_id]\n",
    "        return index\n",
    "\n",
    "    def get_rel_index(rel_id):\n",
    "        if rel_id not in relations:\n",
    "            relations[rel_id] = len(relations)\n",
    "        return relations[rel_id]\n",
    "                \n",
    "    with open(go_file) as f:\n",
    "        for line in f:\n",
    "            line = line.strip().replace('_', ':')\n",
    "            if line.find('SubClassOf') == -1:\n",
    "                continue\n",
    "            left, right = line.split(' SubClassOf ')\n",
    "            # C SubClassOf D\n",
    "            if len(left) == 10 and len(right) == 10:\n",
    "                go1, go2 = left, right\n",
    "                nf1.append((get_index(go1), get_index(go2)))\n",
    "            elif left.find('and') != -1: # C and D SubClassOf E\n",
    "                go1, go2 = left.split(' and ')\n",
    "                go3 = right\n",
    "                nf2.append((get_index(go1), get_index(go2), get_index(go3)))\n",
    "            elif left.find('some') != -1:  # R some C SubClassOf D\n",
    "                rel, go1 = left.split(' some ')\n",
    "                go2 = right\n",
    "                nf3.append((get_rel_index(rel), get_index(go1), get_index(go2)))\n",
    "            elif right.find('some') != -1: # C SubClassOf R some D\n",
    "                go1 = left\n",
    "                rel, go2 = right.split(' some ')\n",
    "                nf4.append((get_index(go1), get_rel_index(rel), get_index(go2)))\n",
    "    return nf1, nf2, nf3, nf4, relations, zclasses \n",
    "    \n",
    "def load_data(data_root, ont, terms_file):\n",
    "    terms_df = pd.read_pickle(terms_file)\n",
    "    terms = terms_df['gos'].values.flatten()\n",
    "    terms_dict = {v: i for i, v in enumerate(terms)}\n",
    "    print('Terms', len(terms))\n",
    "    \n",
    "    ipr_df = pd.read_pickle(f'{data_root}/{ont}/interpros.pkl')\n",
    "    iprs = ipr_df['interpros'].values\n",
    "    iprs_dict = {v:k for k, v in enumerate(iprs)}\n",
    "    return iprs_dict, terms_dict\n",
    "\n",
    "def get_data(df, iprs_dict, terms_dict):\n",
    "    data = th.zeros((len(df), len(iprs_dict)), dtype=th.float32)\n",
    "    labels = th.zeros((len(df), len(terms_dict)), dtype=th.float32)\n",
    "    for i, row in enumerate(df.itertuples()):\n",
    "        #Enumerate over each row in dataframe\n",
    "        for ipr in row.interpros:\n",
    "            # For each ipr in the interpro column of a row, check if its in the dict\n",
    "            # TODO, how is this interpro column made?! is it interproscan?\n",
    "            if ipr in iprs_dict:\n",
    "                data[i, iprs_dict[ipr]] = 1 #if so, add a count\n",
    "        for go_id in row.prop_annotations: # prop_annotations for full model\n",
    "            if go_id in terms_dict:\n",
    "                g_id = terms_dict[go_id]\n",
    "                labels[i, g_id] = 1\n",
    "    return data, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8de5a18e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-11T07:26:59.000889Z",
     "iopub.status.busy": "2023-08-11T07:26:59.000574Z",
     "iopub.status.idle": "2023-08-11T07:26:59.037699Z",
     "shell.execute_reply": "2023-08-11T07:26:59.031921Z"
    },
    "papermill": {
     "duration": 0.049426,
     "end_time": "2023-08-11T07:26:59.040027",
     "exception": false,
     "start_time": "2023-08-11T07:26:58.990601",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from https://github.com/bio-ontology-research-group/deepgozero/blob/main/deepgozero.py\n",
    "# Model definition\n",
    "class Residual(nn.Module):\n",
    "\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.fn(x)\n",
    "    \n",
    "        \n",
    "class MLPBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True, layer_norm=True, dropout=0.1, activation=nn.ReLU):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features, bias)\n",
    "        self.activation = activation()\n",
    "        self.layer_norm = nn.BatchNorm1d(out_features, track_running_stats=False) if layer_norm else None\n",
    "        self.dropout = nn.Dropout(dropout) if dropout else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.linear(x))\n",
    "        if self.layer_norm:\n",
    "            x = self.layer_norm(x)\n",
    "        if self.dropout:\n",
    "            x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DGELModel(nn.Module):\n",
    "\n",
    "    def __init__(self, nb_iprs, nb_gos, nb_zero_gos, nb_rels, device, hidden_dim=1024, embed_dim=1024, margin=0.1):\n",
    "        super().__init__()\n",
    "        self.nb_gos = nb_gos\n",
    "        self.nb_zero_gos = nb_zero_gos\n",
    "        input_length = nb_iprs\n",
    "        net = []\n",
    "        net.append(MLPBlock(input_length, hidden_dim))\n",
    "        net.append(Residual(MLPBlock(hidden_dim, hidden_dim)))\n",
    "        self.net = nn.Sequential(*net)\n",
    "\n",
    "        # ELEmbeddings\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hasFuncIndex = th.LongTensor([nb_rels]).to(device)\n",
    "        self.go_embed = nn.Embedding(nb_gos + nb_zero_gos, embed_dim)\n",
    "        self.go_norm = nn.BatchNorm1d(embed_dim)\n",
    "        k = math.sqrt(1 / embed_dim)\n",
    "        nn.init.uniform_(self.go_embed.weight, -k, k)\n",
    "        self.go_rad = nn.Embedding(nb_gos + nb_zero_gos, 1)\n",
    "        nn.init.uniform_(self.go_rad.weight, -k, k)\n",
    "        # self.go_embed.weight.requires_grad = False\n",
    "        # self.go_rad.weight.requires_grad = False\n",
    "        \n",
    "        self.rel_embed = nn.Embedding(nb_rels + 1, embed_dim)\n",
    "        nn.init.uniform_(self.rel_embed.weight, -k, k)\n",
    "        self.all_gos = th.arange(self.nb_gos).to(device)\n",
    "        self.margin = margin\n",
    "\n",
    "        \n",
    "    def forward(self, features):\n",
    "        x = self.net(features)\n",
    "        go_embed = self.go_embed(self.all_gos)\n",
    "        hasFunc = self.rel_embed(self.hasFuncIndex)\n",
    "        hasFuncGO = go_embed + hasFunc\n",
    "        go_rad = th.abs(self.go_rad(self.all_gos).view(1, -1))\n",
    "        x = th.matmul(x, hasFuncGO.T) + go_rad\n",
    "        logits = th.sigmoid(x)\n",
    "        return logits\n",
    "\n",
    "    def predict_zero(self, features, data):\n",
    "        x = self.net(features)\n",
    "        go_embed = self.go_embed(data)\n",
    "        hasFunc = self.rel_embed(self.hasFuncIndex)\n",
    "        hasFuncGO = go_embed + hasFunc\n",
    "        go_rad = th.abs(self.go_rad(data).view(1, -1))\n",
    "        x = th.matmul(x, hasFuncGO.T) + go_rad\n",
    "        logits = th.sigmoid(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "    def el_loss(self, go_normal_forms):\n",
    "        nf1, nf2, nf3, nf4 = go_normal_forms\n",
    "        nf1_loss = self.nf1_loss(nf1)\n",
    "        nf2_loss = self.nf2_loss(nf2)\n",
    "        nf3_loss = self.nf3_loss(nf3)\n",
    "        nf4_loss = self.nf4_loss(nf4)\n",
    "        # print()\n",
    "        # print(nf1_loss.detach().item(),\n",
    "        #       nf2_loss.detach().item(),\n",
    "        #       nf3_loss.detach().item(),\n",
    "        #       nf4_loss.detach().item())\n",
    "        return nf1_loss + nf3_loss + nf4_loss + nf2_loss\n",
    "\n",
    "    def class_dist(self, data):\n",
    "        c = self.go_norm(self.go_embed(data[:, 0]))\n",
    "        d = self.go_norm(self.go_embed(data[:, 1]))\n",
    "        rc = th.abs(self.go_rad(data[:, 0]))\n",
    "        rd = th.abs(self.go_rad(data[:, 1]))\n",
    "        dist = th.linalg.norm(c - d, dim=1, keepdim=True) + rc - rd\n",
    "        return dist\n",
    "        \n",
    "    def nf1_loss(self, data):\n",
    "        pos_dist = self.class_dist(data)\n",
    "        loss = th.mean(th.relu(pos_dist - self.margin))\n",
    "        return loss\n",
    "\n",
    "    def nf2_loss(self, data):\n",
    "        c = self.go_norm(self.go_embed(data[:, 0]))\n",
    "        d = self.go_norm(self.go_embed(data[:, 1]))\n",
    "        e = self.go_norm(self.go_embed(data[:, 2]))\n",
    "        rc = th.abs(self.go_rad(data[:, 0]))\n",
    "        rd = th.abs(self.go_rad(data[:, 1]))\n",
    "        re = th.abs(self.go_rad(data[:, 2]))\n",
    "        \n",
    "        sr = rc + rd\n",
    "        dst = th.linalg.norm(c - d, dim=1, keepdim=True)\n",
    "        dst2 = th.linalg.norm(e - c, dim=1, keepdim=True)\n",
    "        dst3 = th.linalg.norm(e - d, dim=1, keepdim=True)\n",
    "        loss = th.mean(th.relu(dst - sr - self.margin)\n",
    "                    + th.relu(dst2 - rc - self.margin)\n",
    "                    + th.relu(dst3 - rd - self.margin))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def nf3_loss(self, data):\n",
    "        # R some C subClassOf D\n",
    "        n = data.shape[0]\n",
    "        # rS = self.rel_space(data[:, 0])\n",
    "        # rS = rS.reshape(-1, self.embed_dim, self.embed_dim)\n",
    "        rE = self.rel_embed(data[:, 0])\n",
    "        c = self.go_norm(self.go_embed(data[:, 1]))\n",
    "        d = self.go_norm(self.go_embed(data[:, 2]))\n",
    "        # c = th.matmul(c, rS).reshape(n, -1)\n",
    "        # d = th.matmul(d, rS).reshape(n, -1)\n",
    "        rc = th.abs(self.go_rad(data[:, 1]))\n",
    "        rd = th.abs(self.go_rad(data[:, 2]))\n",
    "        \n",
    "        rSomeC = c + rE\n",
    "        euc = th.linalg.norm(rSomeC - d, dim=1, keepdim=True)\n",
    "        loss = th.mean(th.relu(euc + rc - rd - self.margin))\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def nf4_loss(self, data):\n",
    "        # C subClassOf R some D\n",
    "        n = data.shape[0]\n",
    "        c = self.go_norm(self.go_embed(data[:, 0]))\n",
    "        rE = self.rel_embed(data[:, 1])\n",
    "        d = self.go_norm(self.go_embed(data[:, 2]))\n",
    "        \n",
    "        rc = th.abs(self.go_rad(data[:, 1]))\n",
    "        rd = th.abs(self.go_rad(data[:, 2]))\n",
    "        sr = rc + rd\n",
    "        # c should intersect with d + r\n",
    "        rSomeD = d + rE\n",
    "        dst = th.linalg.norm(c - rSomeD, dim=1, keepdim=True)\n",
    "        loss = th.mean(th.relu(dst - sr - self.margin))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21470353",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-11T07:26:59.058548Z",
     "iopub.status.busy": "2023-08-11T07:26:59.058225Z",
     "iopub.status.idle": "2023-08-11T07:26:59.068067Z",
     "shell.execute_reply": "2023-08-11T07:26:59.067103Z"
    },
    "papermill": {
     "duration": 0.021503,
     "end_time": "2023-08-11T07:26:59.070165",
     "exception": false,
     "start_time": "2023-08-11T07:26:59.048662",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://github.com/bio-ontology-research-group/deepgozero/blob/main/torch_utils.py\n",
    "# Not currently implemented for our test set, but may be necessary given its size.\n",
    "\n",
    "import torch\n",
    "\n",
    "class FastTensorDataLoader:\n",
    "    \"\"\"\n",
    "    A DataLoader-like object for a set of tensors that can be much faster than\n",
    "    TensorDataset + DataLoader because dataloader grabs individual indices of\n",
    "    the dataset and calls cat (slow).\n",
    "    Source: https://discuss.pytorch.org/t/dataloader-much-slower-than-manual-batching/27014/6\n",
    "    \n",
    "    This DataLoader generates tuple outputs of format (tensors,labels)\n",
    "    \"\"\"\n",
    "    def __init__(self, *tensors, batch_size=32, shuffle=False):\n",
    "        \"\"\"\n",
    "        Initialize a FastTensorDataLoader.\n",
    "        :param *tensors: tensors to store. Must have the same length @ dim 0.\n",
    "        :param batch_size: batch size to load.\n",
    "        :param shuffle: if True, shuffle the data *in-place* whenever an\n",
    "            iterator is created out of this object.\n",
    "        :returns: A FastTensorDataLoader.\n",
    "        \"\"\"\n",
    "        assert all(t.shape[0] == tensors[0].shape[0] for t in tensors)\n",
    "        self.tensors = tensors\n",
    "\n",
    "        self.dataset_len = self.tensors[0].shape[0]\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "        # Calculate # batches\n",
    "        n_batches, remainder = divmod(self.dataset_len, self.batch_size)\n",
    "        if remainder > 0:\n",
    "            n_batches += 1\n",
    "        self.n_batches = n_batches\n",
    "    def __iter__(self):\n",
    "        if self.shuffle:\n",
    "            r = torch.randperm(self.dataset_len)\n",
    "            self.tensors = [t[r] for t in self.tensors]\n",
    "        self.i = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.i >= self.dataset_len:\n",
    "            raise StopIteration\n",
    "        batch = tuple(t[self.i:self.i+self.batch_size] for t in self.tensors)\n",
    "        self.i += self.batch_size\n",
    "        return batch\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a3e21e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-11T07:26:59.088678Z",
     "iopub.status.busy": "2023-08-11T07:26:59.088377Z",
     "iopub.status.idle": "2023-08-11T07:27:02.558591Z",
     "shell.execute_reply": "2023-08-11T07:27:02.557600Z"
    },
    "papermill": {
     "duration": 3.48239,
     "end_time": "2023-08-11T07:27:02.561124",
     "exception": false,
     "start_time": "2023-08-11T07:26:59.078734",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terms 10101\n"
     ]
    }
   ],
   "source": [
    "# from https://github.com/bio-ontology-research-group/deepgozero/blob/main/deepgozero.py\n",
    "\n",
    "# Prep dictionary terms\n",
    "loss_func = nn.BCELoss()\n",
    "iprs_dict, terms_dict = load_data(data_root, ont, terms_file)\n",
    "n_terms = len(terms_dict)\n",
    "n_iprs = len(iprs_dict)\n",
    "    \n",
    "nf1, nf2, nf3, nf4, relations, zero_classes = load_normal_forms(go_file, terms_dict)\n",
    "n_rels = len(relations)\n",
    "n_zeros = len(zero_classes)\n",
    "    \n",
    "normal_forms = nf1, nf2, nf3, nf4\n",
    "nf1 = th.LongTensor(nf1).to(device)\n",
    "nf2 = th.LongTensor(nf2).to(device)\n",
    "nf3 = th.LongTensor(nf3).to(device)\n",
    "nf4 = th.LongTensor(nf4).to(device)\n",
    "normal_forms = nf1, nf2, nf3, nf4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4446ded",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-11T07:27:02.580532Z",
     "iopub.status.busy": "2023-08-11T07:27:02.580187Z",
     "iopub.status.idle": "2023-08-11T07:27:02.585238Z",
     "shell.execute_reply": "2023-08-11T07:27:02.584364Z"
    },
    "papermill": {
     "duration": 0.017959,
     "end_time": "2023-08-11T07:27:02.588033",
     "exception": false,
     "start_time": "2023-08-11T07:27:02.570074",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of terms: 10101\n",
      "Number of interpros: 26406\n"
     ]
    }
   ],
   "source": [
    "print('Number of terms:',n_terms)\n",
    "print('Number of interpros:',n_iprs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a62211",
   "metadata": {
    "papermill": {
     "duration": 0.008594,
     "end_time": "2023-08-11T07:27:02.605503",
     "exception": false,
     "start_time": "2023-08-11T07:27:02.596909",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1495c4e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-11T07:27:02.624990Z",
     "iopub.status.busy": "2023-08-11T07:27:02.624638Z",
     "iopub.status.idle": "2023-08-11T07:27:03.763555Z",
     "shell.execute_reply": "2023-08-11T07:27:03.761221Z"
    },
    "papermill": {
     "duration": 1.151173,
     "end_time": "2023-08-11T07:27:03.765767",
     "exception": false,
     "start_time": "2023-08-11T07:27:02.614594",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DGELModel(\n",
      "  (net): Sequential(\n",
      "    (0): MLPBlock(\n",
      "      (linear): Linear(in_features=26406, out_features=1024, bias=True)\n",
      "      (activation): ReLU()\n",
      "      (layer_norm): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): Residual(\n",
      "      (fn): MLPBlock(\n",
      "        (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (activation): ReLU()\n",
      "        (layer_norm): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (go_embed): Embedding(45257, 1024)\n",
      "  (go_norm): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (go_rad): Embedding(45257, 1)\n",
      "  (rel_embed): Embedding(9, 1024)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# initialize\n",
    "net = DGELModel(n_iprs, n_terms, n_zeros, n_rels, device).to(device)\n",
    "print(net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9fc03d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-11T07:27:03.786100Z",
     "iopub.status.busy": "2023-08-11T07:27:03.785773Z",
     "iopub.status.idle": "2023-08-11T07:27:03.792843Z",
     "shell.execute_reply": "2023-08-11T07:27:03.791334Z"
    },
    "papermill": {
     "duration": 0.019664,
     "end_time": "2023-08-11T07:27:03.794864",
     "exception": false,
     "start_time": "2023-08-11T07:27:03.775200",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# If not loading checkpoint, prepare for training\n",
    "if not load:\n",
    "    train_df = pd.read_pickle(f'{data_root}/{ont}/train_data.pkl')\n",
    "    train_data = get_data(train_df, iprs_dict, terms_dict)\n",
    "    print(train_data[0].shape)\n",
    "    train_loader = FastTensorDataLoader(\n",
    "            *train_data, batch_size=batch_size, shuffle=True)\n",
    "    del train_df,train_data \n",
    "\n",
    "    valid_df = pd.read_pickle(f'{data_root}/{ont}/valid_data.pkl')\n",
    "    valid_data = get_data(valid_df, iprs_dict, terms_dict)\n",
    "    print(valid_data[0].shape)\n",
    "    valid_loader = FastTensorDataLoader(\n",
    "            *valid_data, batch_size=batch_size, shuffle=False) \n",
    "\n",
    "    del valid_df, valid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7f02627",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-11T07:27:03.813984Z",
     "iopub.status.busy": "2023-08-11T07:27:03.813158Z",
     "iopub.status.idle": "2023-08-11T07:27:03.989459Z",
     "shell.execute_reply": "2023-08-11T07:27:03.988335Z"
    },
    "papermill": {
     "duration": 0.1889,
     "end_time": "2023-08-11T07:27:03.992466",
     "exception": false,
     "start_time": "2023-08-11T07:27:03.803566",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07f3c762",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-11T07:27:04.011863Z",
     "iopub.status.busy": "2023-08-11T07:27:04.011550Z",
     "iopub.status.idle": "2023-08-11T07:27:04.023230Z",
     "shell.execute_reply": "2023-08-11T07:27:04.022354Z"
    },
    "papermill": {
     "duration": 0.0237,
     "end_time": "2023-08-11T07:27:04.025356",
     "exception": false,
     "start_time": "2023-08-11T07:27:04.001656",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# If not loading checkpoint, prepare for training\n",
    "if not load:\n",
    "    optimizer = th.optim.Adam(net.parameters(), lr=5e-4)\n",
    "    scheduler = MultiStepLR(optimizer, milestones=[5, 20], gamma=0.1)\n",
    "    best_loss = 10000.0\n",
    "\n",
    "    print('Training the model')\n",
    "    for epoch in range(epochs):\n",
    "        net.train()\n",
    "        train_loss = 0\n",
    "        train_elloss = 0\n",
    "        lmbda = 0.1\n",
    "        train_steps = len(train_loader)\n",
    "        for batch_features, batch_labels in train_loader:\n",
    "            batch_features = batch_features.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "            logits = net(batch_features)\n",
    "            loss = F.binary_cross_entropy(logits, batch_labels)\n",
    "            el_loss = net.el_loss(normal_forms)\n",
    "            total_loss = loss + el_loss\n",
    "            train_loss += loss.detach().item()\n",
    "            train_elloss = el_loss.detach().item()\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "                    \n",
    "        train_loss /= train_steps\n",
    "                \n",
    "        net.eval()\n",
    "        with th.no_grad():\n",
    "            valid_steps = len(valid_loader)\n",
    "            valid_loss = 0\n",
    "            preds = []\n",
    "            for batch_features, batch_labels in valid_loader:\n",
    "                batch_features = batch_features.to(device)\n",
    "                batch_labels = batch_labels.to(device)\n",
    "                logits = net(batch_features)\n",
    "                batch_loss = F.binary_cross_entropy(logits, batch_labels)\n",
    "                valid_loss += batch_loss.detach().item()\n",
    "                preds = np.append(preds, logits.detach().cpu().numpy())\n",
    "            valid_loss /= valid_steps\n",
    "            roc_auc = compute_roc(valid_labels, preds)\n",
    "            print(f'Epoch {epoch}: Loss - {train_loss}, EL Loss: {train_elloss}, Valid loss - {valid_loss}, AUC - {roc_auc}')\n",
    "    \n",
    "            print('EL Loss', train_elloss)\n",
    "            if valid_loss < best_loss:\n",
    "                best_loss = valid_loss\n",
    "                print('Saving model')\n",
    "                th.save(net.state_dict(), model_file)\n",
    "\n",
    "            scheduler.step()\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a500b74d",
   "metadata": {
    "papermill": {
     "duration": 0.009257,
     "end_time": "2023-08-11T07:27:04.043464",
     "exception": false,
     "start_time": "2023-08-11T07:27:04.034207",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Inference based on CAFA5 test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b64be74a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-11T07:27:04.063856Z",
     "iopub.status.busy": "2023-08-11T07:27:04.063548Z",
     "iopub.status.idle": "2023-08-11T07:27:04.073919Z",
     "shell.execute_reply": "2023-08-11T07:27:04.073009Z"
    },
    "papermill": {
     "duration": 0.023894,
     "end_time": "2023-08-11T07:27:04.075986",
     "exception": false,
     "start_time": "2023-08-11T07:27:04.052092",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Custom get_data and load_data for CAFA5 test dataset.\n",
    "Note that \n",
    "\n",
    "Major changes\n",
    "\n",
    "get_data: \n",
    "* Convet string interproscan column back into list form (previously generated during runtime so not necessary)\n",
    "* Prepare array input for model\n",
    "* Removed label prep and return\n",
    "* Returning protein ids for submission later\n",
    "\n",
    "load_data:\n",
    "* Take pre-prepared test dataframe as input rather than generation\n",
    "* Removed valid and train df generation\n",
    "* No longer used here because we loaded iprs_dict and terms_dict previously. Saves memory.\n",
    "\n",
    "Recall there are both 3 model files and 3 terms files. Keep in mind for experiments\n",
    "# terms file here in the original approach is /kaggle/input/deepgozero-data/data/mf/terms_zero_10.pkl, but note how there are three sets\n",
    "#- terms_zero_10.pkl\n",
    "#- terms_zero.pkl\n",
    "#- terms.pkl\n",
    "\"\"\"\n",
    "import ast\n",
    "def get_data_test(df, iprs_dict, terms_dict):\n",
    "    df['interproscan']=df['interproscan'].apply(ast.literal_eval) \n",
    "    data = th.zeros((len(df), len(iprs_dict)), dtype=th.float32)\n",
    "    prot_ids = []\n",
    "    for i, row in enumerate(df.itertuples()):\n",
    "        # Add protein id for submission writeup later\n",
    "        prot_id = row.proteins\n",
    "        prot_ids.append(prot_id)\n",
    "        # Add relevant interpro embedding into matrix\n",
    "        for ipr in row.interproscan:\n",
    "            if ipr in iprs_dict:\n",
    "                data[i, iprs_dict[ipr]] = 1\n",
    "               \n",
    "   \n",
    "    return data,prot_ids\n",
    "\n",
    "\n",
    "\n",
    "def load_data_test(data_root, ont, terms_file, df):\n",
    "    terms_df = pd.read_pickle(terms_file)\n",
    "    terms = terms_df['gos'].values.flatten()\n",
    "    terms_dict = {v: i for i, v in enumerate(terms)}\n",
    "    print('Terms', len(terms))\n",
    "    \n",
    "    ipr_df = pd.read_pickle(f'{data_root}/{ont}/interpros.pkl')\n",
    "    iprs = ipr_df['interpros'].values\n",
    "    iprs_dict = {v:k for k, v in enumerate(iprs)}\n",
    "\n",
    "    #df['interproscan']=df['interproscan'].apply(ast.literal_eval) # Convet entries to list, otherwise will just iterate over letters\n",
    "    test_data = get_data_test(df, iprs_dict, terms_dict)\n",
    "    \n",
    "    return iprs_dict, terms_dict, test_data, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9cb1e8f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-11T07:27:04.095336Z",
     "iopub.status.busy": "2023-08-11T07:27:04.094540Z",
     "iopub.status.idle": "2023-08-11T07:27:04.112028Z",
     "shell.execute_reply": "2023-08-11T07:27:04.111167Z"
    },
    "papermill": {
     "duration": 0.029586,
     "end_time": "2023-08-11T07:27:04.114298",
     "exception": false,
     "start_time": "2023-08-11T07:27:04.084712",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# We've loaded iprs_dict and terms_dict previously, so no need to call load_data\n",
    "# iprs_dict: contains all interpro embeddings in the entire dataset (from original repo, not CAFA5, but we'll use it as its trained on it- otherwise input matrix will be of wrong shape)\n",
    "# terms_dict: contains the Ontology specific GO annotations, note these come in different sizes (see hyperparameter optimization)\n",
    "\"\"\"\n",
    "# With default terms, test_data is tensor of torch.Size([138417, 26406]), which  is equal to len(test_df) x len(interpro embeds)\n",
    "test_data, prot_ids= get_data_test(test_df, iprs_dict,terms_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "32884a0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-11T07:27:04.163193Z",
     "iopub.status.busy": "2023-08-11T07:27:04.162262Z",
     "iopub.status.idle": "2023-08-11T07:27:04.167361Z",
     "shell.execute_reply": "2023-08-11T07:27:04.166471Z"
    },
    "papermill": {
     "duration": 0.017424,
     "end_time": "2023-08-11T07:27:04.169393",
     "exception": false,
     "start_time": "2023-08-11T07:27:04.151969",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138417\n"
     ]
    }
   ],
   "source": [
    "print(len(prot_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "899bb029",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-11T07:27:04.189249Z",
     "iopub.status.busy": "2023-08-11T07:27:04.188964Z",
     "iopub.status.idle": "2023-08-11T07:27:54.366741Z",
     "shell.execute_reply": "2023-08-11T07:27:54.365747Z"
    },
    "papermill": {
     "duration": 50.19089,
     "end_time": "2023-08-11T07:27:54.369449",
     "exception": false,
     "start_time": "2023-08-11T07:27:04.178559",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|████████▊                       | 37863/138417 [2:09:08<5:41:34,  4.91it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "go = Ontology(f'{data_root}/go.obo', with_rels=True)\n",
    "\n",
    "# Loading best model\n",
    "print('Loading the best model')\n",
    "net.load_state_dict(th.load(model_file, map_location=device))\n",
    "net.eval()\n",
    "\n",
    "\"\"\"\n",
    "# Original Test evaluation. Not necessary for us, but could be useful for batching if needed.\n",
    "# Set to evaluation mode\n",
    "with th.no_grad():\n",
    "    test_steps = int(math.ceil(len(test_labels) / batch_size))\n",
    "    test_loss = 0\n",
    "    preds = []\n",
    "    # Forward pass test data into modelm calculate BCE as well as ROC AUC\n",
    "    for batch_features, batch_labels in tqdm(test_loader,total=len(test_loader)):\n",
    "        batch_features = batch_features.to(device)\n",
    "        # \n",
    "        print(\"batch features\")\n",
    "        print(batch_features)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "        print(\"batch labels\")\n",
    "        print(batch_labels)\n",
    "        logits = net(batch_features)\n",
    "        batch_loss = F.binary_cross_entropy(logits, batch_labels)\n",
    "        test_loss += batch_loss.detach().cpu().item()\n",
    "        preds = np.append(preds, logits.detach().cpu().numpy())\n",
    "    test_loss /= test_steps\n",
    "    preds = preds.reshape(-1, n_terms)\n",
    "    roc_auc = compute_roc(test_labels, preds)\n",
    "    print(f'Test Loss - {test_loss}, AUC - {roc_auc}')\n",
    "\"\"\"\n",
    "with th.no_grad():\n",
    "\n",
    "    preds = []\n",
    "    batch_features = test_data.to(device)\n",
    "    logits = net(batch_features)   \n",
    "    preds = np.append(preds, logits.detach().cpu().numpy())\n",
    "    preds = preds.reshape(-1, n_terms)\n",
    "    \n",
    "\n",
    "w = open(sub_file, 'wt')\n",
    "preds = list(preds)\n",
    "# Propagate scores using ontology structure\n",
    "# Iterates over each of the score vectors in preds\n",
    "# (len(scores)) with default terms and model setup 10101\n",
    "for i, scores in tqdm(enumerate(preds), total=len(preds)):\n",
    "    \n",
    "    # Use the index in preds to fetch protein id\n",
    "    prot_id = prot_ids[i]\n",
    "    prop_annots = {}\n",
    "    for go_id, j in terms_dict.items():\n",
    "        score = scores[j]\n",
    "        # iterates over the ancestors of a given term in the ontology.\n",
    "        # If an ancestor term already has a score in prop_annots, \n",
    "        # it is updated with the maximum of the current score and the new score. \n",
    "        # If it does not have a score, it is assigned the new score.\n",
    "        for sup_go in go.get_anchestors(go_id):\n",
    "            if sup_go in prop_annots:\n",
    "                prop_annots[sup_go] = max(prop_annots[sup_go], score)\n",
    "            else:\n",
    "                prop_annots[sup_go] = score\n",
    "    # loop over prop_annots.items() \n",
    "    # updates the scores in the original scores vector based on the propagated scores.\n",
    "    for go_id, score in prop_annots.items():\n",
    "        if go_id in terms_dict:\n",
    "            scores[terms_dict[go_id]] = score\n",
    "    #sort them and go over them, looking at thresholds to write to submission file\n",
    "    # For default terms_zero_10, len of prop_annots and sannots is 10490. This is a bit more than 10101 score length, probably due to GO hierarchy.\n",
    "    \n",
    "    sannots = sorted(prop_annots.items(), key=lambda x: x[1], reverse=True)\n",
    "    for go_id, score in sannots:\n",
    "            if score >= threshold:\n",
    "                w.write(prot_id + '\\t' + go_id + '\\t%.3f\\n' % score)\n",
    "    w.write('\\n')\n",
    "w.close()\n",
    "    \n",
    "\n",
    "#  TODO, add diamond /blast results. Existing implementation can be found here https://www.kaggle.com/code/geraseva/deepgoplus under \"Combine diamond preds and deepgo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe50b8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 87.785294,
   "end_time": "2023-08-11T07:27:56.873620",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-08-11T07:26:29.088326",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
